# A Comprehensive Survey on Large Language Model Based Planning

Welcome to the **Awesome-LLM-Planning** repository! This repository is a curated collection of the most influential papers, and benchmarks related to **Large Language Models (LLMs) based Agent Planning**. For a detailed introduction, please refer our survey paper:

**A Comprehensive Survey on Large Language Model Based Planning** 



<p>
  <img src="docs/overview.png" width="95%" height="95%" />
</p>
An overview of LLM-based agent planning, covering its definition, methods, evaluation approaches, and analysis and interpretation.





## 1 Planning Methods

### 1.1 External Module Augmented Methods

#### 1.1.1 Planner Enhanced Methods 

* **LLM+P: Empowering Large Language Models with Optimal Planning Proficiency** [![arXiv](https://img.shields.io/badge/arXiv-2023.04-red)](https://arxiv.org/abs/2304.11477) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Cranial-XIX/llm-pddl)
* **Dynamic Planning With A LLM** [![arXiv](https://img.shields.io/badge/arXiv-2023.08-red)](https://arxiv.org/abs/2308.06391) [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **PDDLEGO: Iterative Planning In Textual Environments** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.19793) 
* **PROC2PDDL: Open-Domain Planning Representations From Texts** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://arxiv.org/abs/2403.00092) 
* **Can Llms Plan Paths With Extra Hints From Solvers?** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.05045) 
* **Robust Planning With Compound LLM Architectures: An LLM-Modulo Approach** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)](https://arxiv.org/abs/2411.14484) 
* **LLM+ Map: Bimanual Robot Task Planning Using Large Language Models And Planning Domain Definition Language** [![arXiv](https://img.shields.io/badge/arXiv-2025.03-red)](https://arxiv.org/abs/2503.17309) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Kchu/LLM-MAP)
* **Explicit Memory Learning With Expectation Maximization** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://aclanthology.org/2024.emnlp-main.927.pdf) 
* **Think Before You Act: Decision Transformers With Working Memory** [![ICML](https://img.shields.io/badge/ICML-2023-red)](https://arxiv.org/abs/2305.16338) 
* **Synapse: Trajectory-As-Exemplar Prompting With Memory For Computer Control** [![ICLR](https://img.shields.io/badge/ICLR-2023-red)](https://arxiv.org/abs/2306.07863) 
* **Generative Agents: Interactive Simulacra Of Human Behavior** [![UIST](https://img.shields.io/badge/UIST-2023-red)](https://arxiv.org/abs/2304.03442) 
* **Memorybank: Enhancing Large Language Models With Long-Term Memory** [![AAAI](https://img.shields.io/badge/AAAI-2024-red)](https://arxiv.org/abs/2305.10250) 
* **Ldm2: A Large Decision Model Imitating Human Cognition With Dynamic Memory Enhancement** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://aclanthology.org/2023.findings-emnlp.309.pdf) 
* **Hierarchical Planning For Complex Tasks With Knowledge Graph-Rag And Symbolic Verification** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.04578) 
* **Z3: An Efficient Smt Solver** [![TACAS](https://img.shields.io/badge/TACAS-2008-red)](https://link.springer.com/chapter/10.1007/978-3-540-78800-3_24) 
* **Leveraging Pre-Trained Large Language Models To Construct And Utilize World Models For Model-Based Task Planning** [![NIPS](https://img.shields.io/badge/nips-2023-red)](https://arxiv.org/abs/2305.14909) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://guansuns.github.io/pages/llm-dm/)
* **Learning Adaptive Planning Representations With Natural Language Guidance** [![arXiv](https://img.shields.io/badge/arXiv-2023-red)](https://arxiv.org/abs/2312.08566) 
* **Trip-Pal: Travel Planning With Guarantees By Combining Large Language Models And Automated Planners** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.10196) 
* **Leveraging Environment Interaction For Automated PDDL Translation And Planning With Large Language Models** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2407.12979) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/BorealisAI/llm-pddl-planning)
* **Coupling Large Language Models With Logic Programming For Robust And General Reasoning From Text** [![ACL](https://img.shields.io/badge/ACL-2023-red)](https://arxiv.org/abs/2307.07696) 
* **To The Globe (Ttg): Towards Language-Driven Guaranteed Travel Planning** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2410.16456) 
* **Position: Llms Canâ€™t Plan, But Can Help Planning In LLM-Modulo Frameworks** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2402.01817) 
* **Translating Natural Language To Planning Goals With Large-Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.02-red)](https://arxiv.org/abs/2302.05128)
* **Planetarium: A Rigorous Benchmark For Translating Text To Structured Planning Languages** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.03321)
* **Autoplanbench: Automatically Generating Benchmarks For LLM Planners From PDDL** [![arXiv](https://img.shields.io/badge/arXiv-2023.11-red)](https://arxiv.org/abs/2311.09830v2)
#### 1.1.2 Memory Enhanced Methods 
* **A Survey On Large Language Model Based Autonomous Agents** [![Front. Comput. Sci](https://img.shields.io/badge/Front.Comput.Sci-2024-red)](https://arxiv.org/abs/2308.11432) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Paitesanshi/LLM-Agent-Survey)
* **The Rise And Potential Of Large Language Model Based Agents: A Survey** [![arXiv](https://img.shields.io/badge/arXiv-2023.09-red)](https://arxiv.org/abs/2309.07864) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/WooooDyy/LLM-Agent-Paper-List)
* **Jarvis-1: Open-World Multi-Task Agents With Memory-Augmented Multimodal Language Models** [![TPAMI](https://img.shields.io/badge/TPAMI-2024-red)](https://arxiv.org/abs/2311.05997) 
* **Think-In-Memory: Recalling And Post-Thinking Enable Llms With Long-Term Memory** [![arXiv](https://img.shields.io/badge/arXiv-2023.11-red)](https://arxiv.org/abs/2311.08719) 
* **Memgpt: Towards Llms As Operating Systems** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.08560) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://www.letta.com/)
* **Large Language Models Are Semi-Parametric Reinforcement Learning Agents** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2306.07929) 
* **Mot: Memory-Of-Thought Enables Chatgpt To Self-Improve** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://aclanthology.org/2023.emnlp-main.392.pdf) 
* **Clin: A Continually Learning Language Agent For Rapid Task Adaptation And Generalization** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.10134) 
* **Ghost In The Minecraft: Generally Capable Agents For Open-World Environments Via Large Language Models With Text-Based Knowledge And Memory** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)](https://arxiv.org/abs/2305.17144) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OpenGVLab/GITM)
* **Open-Ended Instructable Embodied Agents With Memory-Augmented Large Language Models** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://arxiv.org/abs/2310.15127) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://helper-agent-llm.github.io/)
* **Neural Turing Machines** [![arXiv](https://img.shields.io/badge/arXiv-2014.10-red)](https://arxiv.org/abs/1410.5401) 
* **End-To-End Memory Networks** [![NIPS](https://img.shields.io/badge/NIPS-2015-red)](https://arxiv.org/abs/1503.08895) 
* **Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://neurips.cc/media/neurips-2024/Slides/95569.pdf#:~:text=We%20propose%20a%20novel%20ExRAP%20framework%2C%20systematically%20combining,continuous%20instruction%20following%20tasks%20in%20non-stationary%20embodied%20environments.) 
* **Crafting Personalized Agents Through Retrieval-Augmented Generation On Editable Memory Graphs** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2409.19401) 
* **Learning Memory Mechanisms For Decision Making Through Demonstrations** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)](https://arxiv.org/abs/2411.07954) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/WilliamYue37/AttentionTuner)
### 1.2 Finetuning-based Methods
#### 1.2.1 Imitation Learning-based Methods 
* **You Only Look At Screens: Multimodal Chain-Of-Action Agents** [![arXiv](https://img.shields.io/badge/arXiv-2023.09-red)](https://arxiv.org/abs/2309.11436) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/cooelf/Auto-GUI)
* **Multimodal Web Navigation With Instruction-Finetuned Foundation Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)](https://arxiv.org/abs/2305.11854) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://sites.google.com/view/mm-webnav/)
* **Ask-Before-Plan: Proactive Language Agents For Real-World Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://aclanthology.org/2024.findings-emnlp.636.pdf)
* **Agent-Flan: Designing Data And Methods Of Effective Agent Tuning For Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://arxiv.org/abs/2403.12881) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/InternLM/Agent-FLAN)
* **Training Verifiers To Solve Math Word Problems** [![arXiv](https://img.shields.io/badge/arXiv-2021.10-red)](https://arxiv.org/abs/2110.14168) 
* **Fireact: Toward Language Agent Fine-Tuning** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.05915) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://fireact-agent.github.io/)
* **Opencodereasoning: Advancing Data Distillation For Competitive Coding** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.01943) 
* **Distilling Instruction-Following Abilities Of Large Language Models With Task-Aware Curriculum Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.13448)
* **Retrieve-Plan-Generation: An Iterative Planning And Answering Framework For Knowledge-Intensive LLM Generation** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.14979) 
* **Selp: Generating Safe And Efficient Task Plans For Robot Agents With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.19471) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/lt-asset/selp)
* **Horizon-Length Prediction: Advancing Fill-In-The-Middle Capabilities For Code Generation With Lookahead Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.03103) 
* **Cooperative Strategic Planning Enhances Reasoning Capabilities In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.20007#:~:text=This%20paper%20proposes%20a%20novel%20cooperative%20multi-agent%20reasoning,agents%3A%20a%20planning%20agent%20and%20a%20reasoning%20agent.) 
* **Enhancing Reasoning Capabilities Of Llms Via Principled Synthetic Logic Corpus** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2411.12498#:~:text=To%20address%20this%2C%20we%20propose%20Additional%20Logic%20Training,integrating%20symbolic%20logic%20theory%20and%20previous%20empirical%20insights.) 
* **Knowagent: Knowledge-Augmented Planning For LLM-Based Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://arxiv.org/abs/2403.03101) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/KnowAgent)
* **Autoact: Automatic Agent Learning From Scratch Via Self-Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.01-red)](https://arxiv.org/abs/2401.05268) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/AutoAct)
* **Learning To Plan For Retrieval-Augmented Large Language Models From Knowledge Graphs** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.14282) 
* **Adapting LLM Agents With Universal Feedback In Communication** [![ICMLW](https://img.shields.io/badge/ICMLW-2024-red)](https://arxiv.org/abs/2310.01444) 
* **Stream Of Search (Sos): Learning To Search In Language** [![arXiv](https://img.shields.io/badge/arXiv-2024.04-red)](https://arxiv.org/abs/2404.03683) 
* **Dualformer: Controllable Fast And Slow Thinking By Learning With Randomized Reasoning Traces** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.09918#:~:text=To%20address%20this%20challenge%2C%20we%20present%20Dualformer%2C%20a,parts%20of%20the%20traces%20are%20dropped%20during%20training.) 
* **Monte Carlo Tree Search Boosts Reasoning Via Iterative Preference Learning** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.00451) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/YuxiXie/MCTS-DPO)
* **Beyond A\*: Better Planning With Transformers Via Search Dynamics Bootstrapping** [![arXiv](https://img.shields.io/badge/arXiv-2024.02-red)](https://arxiv.org/abs/2402.14083) 
* **Synatra: Turning Indirect Knowledge Into Direct Demonstrations For Digital Agents At Scale** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.15637)
* -------------------------------------------
* **Learning To Plan Long-Term For Language Modeling** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Agentgen: Enhancing Planning Abilities For Large Language Model Based Agent Via Environment And Task Generation** [![arXiv](https://img.shields.io/badge/arXiv-2024.08-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts In Multi-Objective Alignment** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Mixture-Of-Agents Enhances Large Language Model Capabilities** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Camphor: Collaborative Agents For Multi-Input Planning And High-Order Reasoning On Device** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Agent Planning With World Knowledge Model** [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
#### 1.2.2 Feedback-based Methods
* **A Real-World Webagent With Planning, Long Context Understanding, And Program Synthesis** [![arXiv](https://img.shields.io/badge/arXiv-2023.07-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Closed-Loop Long-Horizon Robotic Planning Via Equilibrium Sequence Modeling** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Autowebglm: A Large Language Model-Based Web Navigating Agent** [![SIGKDD](https://img.shields.io/badge/SIGKDD-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Self-Playing Adversarial Language Game Enhances LLM Reasoning** [![arXiv](https://img.shields.io/badge/arXiv-2024.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Mastering Chess And Shogi By Self-Play With A General Reinforcement Learning Algorithm** [![arXiv](https://img.shields.io/badge/arXiv-2017.12-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Advancing LLM Reasoning Generalists With Preference Trees** [![arXiv](https://img.shields.io/badge/arXiv-2024.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Language Models Can Teach Themselves To Program Better** [![arXiv](https://img.shields.io/badge/arXiv-2022.07-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **React Meets Actre: When Language Agents Enjoy Training Data Autonomy.** [![CoRR](https://img.shields.io/badge/CoRR-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Trial And Error: Exploration-Based Trajectory Optimization For LLM Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Llms In The Imaginarium: Tool Learning Through Simulated Trial And Error** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Can LLM Be A Good Path Planner Based On Prompt Engineering? Mitigating The Hallucination For Path Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.08-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Think Smarter Not Harder: Adaptive Reasoning With Inference Aware Optimization** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Boost, Disentangle, And Customize: A Robust System2-To-System1 Pipeline For Code Generation** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Beyond Human Data: Scaling Self-Training For Problem-Solving With Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.12-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Training Language Models To Self-Correct Via Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv--red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Flow Of Reasoning: Training Llms For Divergent Problem Solving With Minimal Examples** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Webrl: Training LLM Web Agents Via Self-Evolving Online Curriculum Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Rest-Mcts\*: LLM Self-Training Via Process Reward Guided Tree Search** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Q\*: Improving Multi-Step Reasoning For Llms With Deliberative Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Learning Planning-Based Reasoning By Trajectories Collection And Process Reward Synthesizing** [![arXiv](https://img.shields.io/badge/arXiv-2024.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Gui-R1: A Generalist R1-Style Vision-Language Action Model For Gui Agents** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Ui-R1: Enhancing Action Prediction Of Gui Agents By Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2025.03-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Infigui-R1: Advancing Multimodal Gui Agents From Reactive Actors To Deliberative Reasoners** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Embodied-R: Collaborative Framework For Activating Embodied Spatial Reasoning In Foundation Models Via Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Retool: Reinforcement Learning For Strategic Tool Use In Llms** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Toolrl: Reward Is All Tool Learning Needs** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Otc: Optimal Tool Calls Via Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Alphamaze: Enhancing Large Language Models' Spatial Intelligence Via Grpo** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Swe-Rl: Advancing LLM Reasoning Via Reinforcement Learning On Open Software Evolution** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Self-Play Fine-Tuning Converts Weak Language Models To Strong Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.01-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Learn Beyond The Answer: Training Language Models With Reflection For Mathematical Reasoning** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Self-Play Preference Optimization For Language Model Alignment** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Chain Of Preference Optimization: Improving Chain-Of-Thought Reasoning In Llms** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Cpl: Critical Plan Step Learning Boosts LLM Generalization In Reasoning Tasks** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Enhancing LLM Reasoning Via Critique Models With Test-Time And Training-Time Supervision** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Thinking Llms: General Instruction Following With Thought Generation** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Error Classification Of Large Language Models On Math Word Problems: A Dynamically Adaptive Framework** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
### 1.3 Searching-based Methods
#### 1.3.1 Decomposition-based Methods
* **Rada: Retrieval-Augmented Web Agent Planning With Llms** [![ACL](https://img.shields.io/badge/ACL-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Meta-Task Planning For Language Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Agent-Oriented Planning In Multi-Agent Systems** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Deductive Additivity For Planning Of Natural Language Proofs** [![arXiv](https://img.shields.io/badge/arXiv-2023.07-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Paradise: Evaluating Implicit Planning Skills Of Language Models With Procedural Warnings And Tips Dataset** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Optimizing Chain-Of-Thought Reasoning: Tackling Arranging Bottleneck Via Plan Augmentation** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Least-To-Most Prompting Enables Complex Reasoning In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2022.05-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Protrix: Building Models For Planning And Reasoning Over Tables With Sentence Context** [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Hugginggpt: Solving AI Tasks With Chatgpt And Its Friends In Hugging Face** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Urbanllm: Autonomous Urban Activity Planning And Management With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Thoughts To Target: Enhance Planning For Target-Driven Conversation** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Plan-Rag: Planning-Guided Retrieval Augmented Generation** [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Distilling Script Knowledge From Large Language Models For Constrained Language Planning** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **A Human-Like Reasoning Framework For Multi-Phases Planning Task With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Interpretable Math Word Problem Solution Generation Via Step-By-Step Planning** [![arXiv](https://img.shields.io/badge/arXiv-2023.06-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Oscar: Operating System Control Via State-Aware Reasoning And Re-Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Msi-Agent: Incorporating Multi-Scale Insight Into Embodied Agents For Superior Planning And Decision-Making** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Verilogcoder: Autonomous Verilog Coding Agents With Graph-Based Planning And Abstract Syntax Tree (Ast)-Based Waveform Tracing Tool** [![AAAI](https://img.shields.io/badge/AAAI-2025-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Can We Further Elicit Reasoning In Llms? Critic-Guided Planning With Retrieval-Augmentation For Solving Challenging Tasks** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Personal Large Language Model Agents: A Case Study On Tailored Travel Planning** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Strength Lies In Differences! Improving Strategy Planning For Non-Collaborative Dialogues Via Diversified User Simulation** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Adapt: As-Needed Decomposition And Planning With Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.11-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Travelagent: An AI Assistant For Personalized Travel Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
#### 1.3.2 Exploration-based Methods
* **Thinking, Fast And Slow** [![macmillan](https://img.shields.io/badge/macmillan-2011-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **The Empirical Case For Two Systems Of Reasoning.** [![APA](https://img.shields.io/badge/APA-1996-red)]()
* **Flow-Of-Options: Diversified And Improved LLM Reasoning By Thinking Through Options** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Tree Of Thoughts: Deliberate Problem Solving With Large Language Models** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Search-In-The-Chain: Interactively Enhancing Large Language Models With Search For Knowledge-Intensive Tasks** [![WWW](https://img.shields.io/badge/WWW-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Tree-Planner: Efficient Close-Loop Task Planning With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Graph Of Thoughts: Solving Elaborate Problems With Large Language Models** [![AAAI](https://img.shields.io/badge/AAAI-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Thought Propagation: An Analogical Approach To Complex Reasoning With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Avis: Autonomous Visual Information Seeking With Large Language Model Agent** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Toolllm: Facilitating Large Language Models To Master 16000+ Real-World Apis** [![arXiv](https://img.shields.io/badge/arXiv-2023.07-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Algorithm Of Thoughts: Enhancing Exploration Of Ideas In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.08-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **System-1. X: Learning To Balance Fast And Slow Planning With Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Adaptive Graph Of Thoughts: Test-Time Adaptive Reasoning Unifying Chain, Tree, And Graph Structures** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Step Back To Leap Forward: Self-Backtracking For Boosting Reasoning Of Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Codetree: Agent-Guided Tree Search For Code Generation With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Efficient Selectivity And Backup Operators In Monte-Carlo Tree Search** [![ICCG](https://img.shields.io/badge/ICCG-2006-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Bandit Based Monte-Carlo Planning** [![ECML](https://img.shields.io/badge/ECML-2006-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Large Language Models As Commonsense Knowledge For Large-Scale Task Planning** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Language Agent Tree Search Unifies Reasoning Acting And Planning In Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Alphamath Almost Zero: Process Supervision Without Process** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Doubly Robust Monte Carlo Tree Search** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Thoughtsculpt: Reasoning With Intermediate Revision And Search** [![arXiv](https://img.shields.io/badge/arXiv-2024.04-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Monte Carlo Planning With Large Language Model For Text-Based Games** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Coat: Chain-Of-Associated-Thoughts Framework For Enhancing Large Language Models Reasoning** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Webpilot: A Versatile And Autonomous Multi-Agent System For Web Task Execution With Strategic Exploration** [![arXiv](https://img.shields.io/badge/arXiv-2024.08-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **A Formal Basis For The Heuristic Determination Of Minimum Cost Paths** [![SMC](https://img.shields.io/badge/SMC-1968-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Toolchain*: Efficient Action Space Navigation In Large Language Models With A* Search** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Tree Search For Language Model Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **LLM-A\*: Large Language Model Enhanced Incremental Heuristic Search On Path Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **World Models** [![arXiv](https://img.shields.io/badge/arXiv-2018.03-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Deep Learning, Reinforcement Learning, And World Models** [![NN](https://img.shields.io/badge/NN-2022-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Reasoning With Language Model Is Planning With World Model** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Is Your LLM Secretly A World Model Of The Internet? Model-Based Planning For Web Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Reflective Planning: Vision-Language Models For Multi-Stage Long-Horizon Robotic Manipulation** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Worldcoder, A Model-Based LLM Agent: Building World Models By Writing Code And Interacting With The Environment** [![NIPS](https://img.shields.io/badge/NIPS-2025-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Reasonplanner: Enhancing Autonomous Planning In Dynamic Environments With Temporal Knowledge Graphs And Llms** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Solving Math Word Problems With Process-And Outcome-Based Feedback** [![arXiv](https://img.shields.io/badge/arXiv-2022.11-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Let'S Verify Step By Step** [![ICLR](https://img.shields.io/badge/ICLR-2023-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Qlass: Boosting Language Agent Inference Via Q-Guided Stepwise Search** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Scaling Autonomous Agents Via Automatic Reward Modeling And Planning** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
#### 1.3.3 Decoding-based Methods
* **Speech And Language Processing: An Introduction To Natural Language Processing, Computational Linguistics, And Speech Recognition** [![Citeseer](https://img.shields.io/badge/aCiteseer-2008-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Sequence Transduction With Recurrent Neural Networks** [![arXiv](https://img.shields.io/badge/arXiv-2012.11-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Self-Evaluation Guided Beam Search For Reasoning** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Chain-Of-Thought Reasoning Without Prompting** [![arXiv](https://img.shields.io/badge/arXiv-2024.02-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Flap: Flow-Adhering Planning With Constrained Decoding In Llms** [![ACL](https://img.shields.io/badge/ACL-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Non-Myopic Generation Of Language Model For Reasoning And Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Chain-Of-Thought Prompting Elicits Reasoning In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2022-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Don't Throw Away Your Value Model! Generating More Preferable Text With Value-Guided Monte-Carlo Tree Search Decoding** [![CoLM](https://img.shields.io/badge/CoLM-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Contrastive Decoding Improves Reasoning In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.09-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Grounded Decoding: Guiding Text Generation With Grounded Models For Embodied Agents** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()

## 2 Planning Evaluation
### 2.1 Datasets
#### 2.1.1 Digital Scenarios
##### 2.1.1.1 Web Navigation
* **WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents** [![NIPS](https://img.shields.io/badge/NIPS-2022-red)](https://arxiv.org/abs/2207.01206) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/princeton-nlp/WebShop)
* **Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration** [![ICLR](https://img.shields.io/badge/ICLR-2018-red)](https://arxiv.org/abs/1802.08802) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Farama-Foundation/miniwob-plusplus)
* **Mind2Web: Towards a Generalist Agent for the Web** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2306.06070) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OSU-NLP-Group/Mind2Web)
* **WebArena: A Realistic Web Environment for Building Autonomous Agents** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2307.13854) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/web-arena-x/webarena)
* **VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2401.13649) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/web-arena-x/visualwebarena)
* **WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2401.13919) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/MinorJerry/WebVoyager)
* **WebLINX: Real-World Website Navigation with Multi-Turn Dialogue** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2402.05930) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/McGill-NLP/weblinx)

##### 2.1.1.2 Mobile Navigation
* **AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2405.14573) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-research/android_world)
* **Mapping Natural Language Instructions to Mobile UI Action Sequences** [![ACL](https://img.shields.io/badge/ACL-2020-red)](https://arxiv.org/abs/2005.03776) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-research/google-research/tree/master/seq2act)
* **META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI** [![EMNLP](https://img.shields.io/badge/EMNLP-2022-red)](https://arxiv.org/abs/2205.11029) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://x-lance.github.io/META-GUI-Leaderboard/)
* **Android in the Wild: A Large-Scale Dataset for Android Device Control** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2307.10088) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-research/google-research/tree/master/android_in_the_wild)
* **On the Effects of Data Scale on UI Control Agents** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2406.03679) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-research/google-research/tree/master/android_control)
* **Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)](https://arxiv.org/abs/2305.08144) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/X-LANCE/Mobile-Env)
* **A3: Android Agent Arena for Mobile GUI Agents**  [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.01149) 

##### 2.1.1.3 Desktop Navigation
* **OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2404.07972) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/xlang-ai/OSWorld)
* **AgentStudio: A Toolkit for Building General Virtual Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2403.17918) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/ltzheng/agent-studio)
* **TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.14161) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/TheAgentCompany/TheAgentCompany)
* **Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.08264) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/WindowsAgentArena)
* **WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2403.07718) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/ServiceNow/WorkArena)

#### 2.1.2 Embodied Scenarios
##### 2.1.2.1 Household Robot
* **ALFWorld: Aligning Text and Embodied Environments for Interactive Learning** [![ICLR](https://img.shields.io/badge/ICLR-2021-red)](https://arxiv.org/abs/2010.03768) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/alfworld/alfworld)
* **ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks** [![CVPR](https://img.shields.io/badge/CVPR-2020-red)](https://arxiv.org/abs/1912.01734) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/askforalfred/alfred)
* **VirtualHome: Simulating Household Activities via Programs** [![CVPR](https://img.shields.io/badge/CVPR-2018-red)](https://arxiv.org/abs/1806.07011) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/xavierpuigf/virtualhome)
* **ScienceWorld: Is your Agent Smarter than a 5th Grader?** [![EMNLP](https://img.shields.io/badge/EMNLP-2022-red)](https://arxiv.org/abs/2203.07540) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/allenai/ScienceWorld)
* **Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration** [![ICLR](https://img.shields.io/badge/ICLR-2021-red)](https://arxiv.org/abs/2010.09890) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/xavierpuigf/watch_and_help)
* **LangSuitE: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2406.16294) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/bigai-nlco/langsuite)
* **ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2410.03907) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/HKUST-KnowComp/ActPlan-1K)
* **PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2411.00081) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](http://github.com/facebookresearch/partnr-planner)
* **Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2410.07166) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/embodied-agent-interface/embodied-agent-interface)
* **LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2402.08178) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/lbaa2022/LLMTaskPlanning)
* **GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation** [![CVPR](https://img.shields.io/badge/CVPR-2024-red)](https://arxiv.org/abs/2404.06609) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Ram81/goat-bench)
* **BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation** [![CoRL](https://img.shields.io/badge/CoRL-2022-red)](https://arxiv.org/abs/2403.09227) [![Code](https://img.shields.io/badge/Code-Dataset-blue)](https://behavior.stanford.edu/behavior-1k)

##### 2.1.2.2 Manipulation Robot
* **VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation** [![NIPS](https://img.shields.io/badge/NIPS-2022-red)](https://arxiv.org/abs/2206.08522) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/eric-ai-lab/vlmbench)
* **VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.18194) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OpenMOSS/VLABench)
* **VIMA: General Robot Manipulation with Multimodal Prompts** [![ICML](https://img.shields.io/badge/ICML-2023-red)](https://arxiv.org/abs/2210.03094) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/vimalabs/VIMA)
* **EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents** [![ICML](https://img.shields.io/badge/ICML-2025-red)](https://arxiv.org/abs/2502.09560) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/EmbodiedBench/EmbodiedBench)
* **RoCo: Dialectic Multi-Robot Collaboration with Large Language Models** [![ICRA](https://img.shields.io/badge/ICRA-2024-red)](https://arxiv.org/abs/2307.04738) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/MandiZhao/robot-collab)

##### 2.1.2.3 Minecraft Robot
* **MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge** [![NIPS](https://img.shields.io/badge/NIPS-2022-red)](https://arxiv.org/abs/2206.08853) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/MineDojo/MineDojo)
* **Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2402.16801) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/MichaelTMatthews/Craftax)
* **MinePlanner: A Benchmark for Long-Horizon Planning in Large Minecraft Worlds** [![ICAPS](https://img.shields.io/badge/ICAPS-2024-red)](https://arxiv.org/abs/2312.12891) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/IretonLiu/mine-pddl/)
* **TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.05255) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/teamcraft-bench/teamcraft)
* **Plancraft: an evaluation dataset for planning with LLM agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.21033) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/gautierdag/plancraft)
* **MindAgent: Emergent Gaming Interaction** [![arXiv](https://img.shields.io/badge/arXiv-2023.09-red)](https://arxiv.org/abs/2309.09971) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/mindagent/mindagent)
* **On the Utility of Learning about Humans for Human-AI Coordination** [![NIPS](https://img.shields.io/badge/NIPS-2019-red)](https://arxiv.org/abs/1910.05789) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/HumanCompatibleAI/overcooked_ai)

##### 2.1.2.4 Autonomous Driving
* **PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2402.15527) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/pkunlp-icler/PCA-EVAL)
* **AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning** [![arXiv](https://img.shields.io/badge/arXiv-2025.03-red)](https://arxiv.org/abs/2503.07608) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/hustvl/AlphaDrive)

#### 2.1.3 Everyday Scenarios
##### 2.1.3.1 Travel Planning
* **TravelPlanner: A Benchmark for Real-World Planning with Language Agents** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2402.01622) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OSU-NLP-Group/TravelPlanner)
* **ChinaTravel: A Real-World Benchmark for Language Agents in Chinese Travel Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.13682) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/LAMDASZ-ML/ChinaTravel)
* **NATURAL PLAN: Benchmarking LLMs on Natural Language Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.04520) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-deepmind/natural-plan)

##### 2.1.3.2 Workflow
* **FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2406.14884) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Justherozen/FlowBench)
* **Open Grounded Planning: Challenges and Benchmark Construction** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2406.02903) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Shiguang-Guo/Open-Grounded-Planning)
* **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2303.17580) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/JARVIS/tree/main/hugginggpt)
* **TaskBench: Benchmarking Large Language Models for Task Automation** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2311.18760) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/JARVIS/tree/main/taskbench)
* **TaskLAMA: Probing the Complex Task Understanding of Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.08-red)](https://arxiv.org/abs/2308.15299) [![Code](https://img.shields.io/badge/Code-Dataset-blue)](https://storage.googleapis.com/gresearch/tasklama/tasklama.zip)
* **Benchmarking Agentic Workflow Generation** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2410.07869) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/WorfBench)
* **Multimodal Procedural Planning via Dual Text-Image Prompting** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)](https://arxiv.org/abs/2305.01795) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/YujieLu10/TIP)
* **Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2411.17188) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Dongping-Chen/ISG)

##### 2.1.3.3 Tool Calling
* **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2307.16789) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OpenBMB/ToolBench)
* **AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2407.18901) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/stonybrooknlp/appworld)
* **API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://arxiv.org/abs/2304.08244) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank)
* **ToolComp: A Multi-Tool Reasoning & Process Supervision Benchmark** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.01290)
* **ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities** [![arXiv](https://img.shields.io/badge/arXiv-2024.08-red)](https://arxiv.org/abs/2408.04682) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/apple/ToolSandbox)
* **ToolTalk: Evaluating Tool-Usage in a Conversational Setting** [![arXiv](https://img.shields.io/badge/arXiv-2023.11-red)](https://arxiv.org/abs/2311.10775) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/ToolTalk)

##### 2.1.3.4 Code Generation
* **SWE-bench: Can Language Models Resolve Real-World GitHub Issues?** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2310.06770) [![Code](https://img.shields.io/badge/Code-Dataset-blue)](https://www.swebench.com/)
* **Training Software Engineering Agents and Verifiers with SWE-Gym** [![ICML](https://img.shields.io/badge/ICML-2025-red)](https://arxiv.org/abs/2412.21139) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/SWE-Gym/SWE-Gym)
* **Evaluating Large Language Models Trained on Code** [![arXiv](https://img.shields.io/badge/arXiv-2021.07-red)](https://arxiv.org/abs/2107.03374) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/openai/human-eval)

##### 2.1.3.5 Game Playing
* **VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.01863) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning)
* **TextWorld: A Learning Environment for Text-based Games** [![IJCAI](https://img.shields.io/badge/IJCAI-2018-red)](https://arxiv.org/abs/1806.11532) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/microsoft/TextWorld)
* **BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning** [![ICLR](https://img.shields.io/badge/ICLR-2019-red)](https://arxiv.org/abs/1810.08272) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/mila-iqia/babyai/tree/iclr19)
* **PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2206.10498) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/karthikv792/LLMs-Planning/tree/main/plan-bench)

#### 2.1.4 Vertical Scenarios
##### 2.1.4.1 Machine Learning
* **MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2410.07095) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/openai/mle-bench/)

##### 2.1.4.2 AI Research
* **ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.10291)
* **CycleResearcher: Improving Automated Research via Automated Review** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2411.00816) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://wengsyx.github.io/Researcher/)

##### 2.1.4.3 Biological Research
* **BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://arxiv.org/abs/2310.10632) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/bioplanner/bioplanner)

##### 2.1.4.4 Financial Simulation
* **Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.05746) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/jiangjiechen/auction-arena)

##### 2.1.4.5 Interior Design
* **DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.15723) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/plstory/DS2D)
* **Tell2Design: A Dataset for Language-Guided Floor Plan Generation** [![ACL](https://img.shields.io/badge/ACL-2023-red)](https://arxiv.org/abs/2311.15941) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/LengSicong/Tell2Design)

##### 2.1.4.6 Comprehensive
* **AgentBench: Evaluating LLMs as Agents** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2308.03688) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/AgentBench)

* **VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.08-red)](https://arxiv.org/abs/2408.06327) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/VisualAgentBench)

### 2.2 Evaluation Metrics

<p align="center">
  <img src="docs/sankeydiagram.png" width="95%" height="95%" />
</p>
The corresponding relationship between planning evaluation metrics and some typical planning datasets. The left three columns represent evaluation metrics of different granularities, while the rightmost column denotes the dataset.

### 2.3 Performance Comparisons
#### 2.3.1 Web Navigation Performance
* **TongUI: Building Generalized GUI Agents by Learning from Multimodal Web Tutorials** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.12679) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/TongUI-agent/TongUI-agent?tab=readme-ov-file)
* **SpiritSight Agent: Advanced GUI Agent with One Look** [![CVPR](https://img.shields.io/badge/CVPR-2025-red)](https://arxiv.org/abs/2503.03196) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://hzhiyuan.github.io/SpiritSight-Agent/)
* **UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.11257)
* **MP-GUI: Modality Perception with MLLMs for GUI Understanding** [![CVPR](https://img.shields.io/badge/CVPR-2025-red)](https://arxiv.org/abs/2503.14021) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/BigTaige/MP-GUI)
* **Magma: A Foundation Model for Multimodal AI Agents** [![CVPR](https://img.shields.io/badge/CVPR-2025-red)](https://arxiv.org/abs/2502.13130) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/microsoft/Magma)
* **Digi-Q: Learning Q-Value Functions for Training Device-Control Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2502.15760) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/DigiRL-agent/digiq)
* **UI-TARS: Pioneering Automated GUI Interaction with Native Agents** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.12326) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/bytedance/UI-TARS)
* **Tree Search for Language Model Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.01476) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/kohjingyu/search-agents)
* **GUICourse: From General Vision Language Models to Versatile GUI Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.11317) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/RUCBM/GUICourse)
* **MiniCPM-V: A GPT-4V Level MLLM on Your Phone** [![CVPR](https://img.shields.io/badge/CVPR-2025-red)](https://arxiv.org/abs/2408.01800) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/OpenBMB/MiniCPM-o)
* **Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2410.05243) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/OSU-NLP-Group/UGround)
* **GPT-4V(ision) is a Generalist Web Agent, if Grounded** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2401.01614) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/OSU-NLP-Group/SeeAct)
* **SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2401.10935) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/njucckevin/SeeClick)
* **CogAgent: A Visual Language Model for GUI Agents** [![CVPR](https://img.shields.io/badge/CVPR-2024-red)](https://arxiv.org/abs/2312.08914) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/CogAgent)
* **Agent Lumos: Unified and Modular Training for Open-Source Language Agents** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2311.05657) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/allenai/lumos)
* **AgentTuning: Enabling Generalized Agent Abilities for LLMs** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.12823) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/AgentTuning)
* **Mind2Web: Towards a Generalist Agent for the Web** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2306.06070) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OSU-NLP-Group/Mind2Web)
* **Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2306.07863) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/ltzheng/Synapse)

<p align="center">
  <img src="docs/WebNavigationPerformance.png" width="95%" height="95%" />
</p>
The performance comparison of different models and methods in web navigation.The value of Mind2Web is the average step success rate of the three subsets.The value of Webarena is task successrate.The value of AITW is step success rate of the subsets.The value of ScreenSpot is step success rate.

#### 2.3.2 Embodied Scenarios Performance
* **ATLaS: Agent Tuning via Learning Critical Steps** [![arXiv](https://img.shields.io/badge/arXiv-2025.03-red)](https://arxiv.org/abs/2503.02197)
* **DebFlow: Automating Agent Creation via Agent Debate** [![CoLM](https://img.shields.io/badge/CoLM-2025-red)](https://arxiv.org/abs/2503.23781)
* **Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.11425) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/ByteDance-Seed/Agent-R)
* **AgentRefine: Enhancing Agent Generalization through Refinement Tuning** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2501.01702) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Fu-Dayuan/AgentRefine)
* **AgentGym: Evolving Large Language Model-based Agents across Diverse Environments** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.04151) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/WooooDyy/AgentGym)
* **Agent Planning with World Knowledge Model** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2405.14205) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/WKM)
* **Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2403.02502) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Yifan-Song793/ETO)
* **KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents** [![NAACL](https://img.shields.io/badge/NAACL-2025-red)](https://arxiv.org/abs/2403.03101) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/KnowAgent)
* **AgentTuning: Enabling Generalized Agent Abilities for LLMs** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.12823) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/AgentTuning)

<p align="center">
  <img src="docs/EmbodiedScenariosPerformance.png" width="65%" height="65%" />
</p>
The performance comparison of different models and methods in embodied.This refers to the average of seen and unseen in the original paper, or the value reported in the original paper.

## 3 Analysis And Interpretation
### 3.1 External Interpretation
* **The pitfalls of next-token prediction** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2403.06963) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/gregorbachmann/Next-Token-Failures)
* **Revealing the Barriers of Language Agents in Planning** [![NAACL](https://img.shields.io/badge/NAACL-2025-red)](https://arxiv.org/abs/2410.12409) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/hsaest/Agent-Planning-Analysis)
* **To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2409.12183) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/Zayne-sprague/To-CoT-or-not-to-CoT)
* **Chain of Thoughtlessness? An Analysis of CoT in Planning** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2405.04776) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/karthikv792/cot-planning)
* **Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.02-red)](https://arxiv.org/abs/2402.12563) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/MBZUAI-CLeaR/IoE-Prompting)
* **Self-Refine: Iterative Refinement with Self-Feedback** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2303.17651) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/madaan/self-refine)
* **Small Language Models Need Strong Verifiers to Self-Correct Reasoning** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2404.17140) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/yunx-z/SCORE)
* **A Theoretical Understanding of Self-Correction through In-context Alignment** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2405.18634) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/yifeiwang77/Self-Correction)

### 3.2 Internal Interpretation
* **Do Large Language Models Latently Perform Multi-Hop Reasoning?** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2402.16837) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/google-deepmind/latent-multi-hop-reasoning)
* **Do language models plan ahead for future tokens?** [![CoLM](https://img.shields.io/badge/CoLM-2024-red)](https://arxiv.org/abs/2404.00859) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/wiwu2390/FutureGPT2-public)
* **Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2406.16033)
* **ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2405.09220)
* **Iteration Head: A Mechanistic Study of Chain-of-Thought** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2406.02128)

<p align="center">
  <img src="docs/interpretation.png" width="50%" height="50%" />
</p>
The illustration of external interpretation and internal interpretation, respectively.