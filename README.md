# A Comprehensive Survey on Large Language Model Based Planning

Welcome to the **Awesome-LLM-Planning** repository! This repository is a curated collection of the most influential papers, and benchmarks related to **Large Language Models (LLMs) based Agent Planning**. For a detailed introduction, please refer our survey paper:

**A Comprehensive Survey on Large Language Model Based Planning** 



<p>
  <img src="docs/overview.png" width="95%" height="95%" />
</p>
An overview of LLM-based agent planning, covering its definition, methods, evaluation approaches, and analysis and interpretation.





## 1 Planning Methods

### 1.1 External Module Augmented Methods


<p>
<iframe src="docs/exexternal_module.pdf"
 width="800px" height="2400px"></iframe>
</p>

#### 1.1.1 Planner Enhanced Methods 

* **Hierarchical Planning For Complex Tasks With Knowledge Graph-Rag And Symbolic Verification** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.04578) 
* **LLM+ Map: Bimanual Robot Task Planning Using Large Language Models And Planning Domain Definition Language** [![arXiv](https://img.shields.io/badge/arXiv-2025.03-red)](https://arxiv.org/abs/2503.17309) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Kchu/LLM-MAP)
* **Robust Planning With Compound LLM Architectures: An LLM-Modulo Approach** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)](https://arxiv.org/abs/2411.14484) 
* **Can Llms Plan Paths With Extra Hints From Solvers?** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.05045) 
* **Planetarium: A Rigorous Benchmark For Translating Text To Structured Planning Languages** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.03321)
* **Trip-Pal: Travel Planning With Guarantees By Combining Large Language Models And Automated Planners** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.10196) 
* **PDDLEGO: Iterative Planning In Textual Environments** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.19793) 
* **PROC2PDDL: Open-Domain Planning Representations From Texts** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://arxiv.org/abs/2403.00092) 
* **Leveraging Environment Interaction For Automated PDDL Translation And Planning With Large Language Models** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2407.12979) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/BorealisAI/llm-pddl-planning) 
* **Position: Llms Canâ€™t Plan, But Can Help Planning In LLM-Modulo Frameworks** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2402.01817)
* **Memorybank: Enhancing Large Language Models With Long-Term Memory** [![AAAI](https://img.shields.io/badge/AAAI-2024-red)](https://arxiv.org/abs/2305.10250)
* **Explicit Memory Learning With Expectation Maximization** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://aclanthology.org/2024.emnlp-main.927.pdf) 
* **To The Globe (Ttg): Towards Language-Driven Guaranteed Travel Planning** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2410.16456)
* **Autoplanbench: Automatically Generating Benchmarks For LLM Planners From PDDL** [![arXiv](https://img.shields.io/badge/arXiv-2023.11-red)](https://arxiv.org/abs/2311.09830v2)
* **Dynamic Planning With A LLM** [![arXiv](https://img.shields.io/badge/arXiv-2023.08-red)](https://arxiv.org/abs/2308.06391) [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **LLM+P: Empowering Large Language Models with Optimal Planning Proficiency** [![arXiv](https://img.shields.io/badge/arXiv-2023.04-red)](https://arxiv.org/abs/2304.11477) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Cranial-XIX/llm-pddl)
* **Translating Natural Language To Planning Goals With Large-Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.02-red)](https://arxiv.org/abs/2302.05128)
* **Generative Agents: Interactive Simulacra Of Human Behavior** [![UIST](https://img.shields.io/badge/UIST-2023-red)](https://arxiv.org/abs/2304.03442) 
* **Think Before You Act: Decision Transformers With Working Memory** [![ICML](https://img.shields.io/badge/ICML-2023-red)](https://arxiv.org/abs/2305.16338) 
* **Leveraging Pre-Trained Large Language Models To Construct And Utilize World Models For Model-Based Task Planning** [![NIPS](https://img.shields.io/badge/nips-2023-red)](https://arxiv.org/abs/2305.14909) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://guansuns.github.io/pages/llm-dm/)
* **Synapse: Trajectory-As-Exemplar Prompting With Memory For Computer Control** [![ICLR](https://img.shields.io/badge/ICLR-2023-red)](https://arxiv.org/abs/2306.07863) 
* **Coupling Large Language Models With Logic Programming For Robust And General Reasoning From Text** [![ACL](https://img.shields.io/badge/ACL-2023-red)](https://arxiv.org/abs/2307.07696) 
* **Ldm2: A Large Decision Model Imitating Human Cognition With Dynamic Memory Enhancement** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://aclanthology.org/2023.findings-emnlp.309.pdf) 
* **Learning Adaptive Planning Representations With Natural Language Guidance** [![arXiv](https://img.shields.io/badge/arXiv-2023-red)](https://arxiv.org/abs/2312.08566) 

#### 1.1.2 Memory Enhanced Methods 

* **Crafting Personalized Agents Through Retrieval-Augmented Generation On Editable Memory Graphs** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2409.19401) 
* **Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://neurips.cc/media/neurips-2024/Slides/95569.pdf#:~:text=We%20propose%20a%20novel%20ExRAP%20framework%2C%20systematically%20combining,continuous%20instruction%20following%20tasks%20in%20non-stationary%20embodied%20environments.) 
* **Large Language Models Are Semi-Parametric Reinforcement Learning Agents** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2306.07929) 
* **A Survey On Large Language Model Based Autonomous Agents** [![Front. Comput. Sci](https://img.shields.io/badge/Front.Comput.Sci-2024-red)](https://arxiv.org/abs/2308.11432) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Paitesanshi/LLM-Agent-Survey)
* **Jarvis-1: Open-World Multi-Task Agents With Memory-Augmented Multimodal Language Models** [![TPAMI](https://img.shields.io/badge/TPAMI-2024-red)](https://arxiv.org/abs/2311.05997) 
* **Learning Memory Mechanisms For Decision Making Through Demonstrations** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)](https://arxiv.org/abs/2411.07954) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/WilliamYue37/AttentionTuner)
* **The Rise And Potential Of Large Language Model Based Agents: A Survey** [![arXiv](https://img.shields.io/badge/arXiv-2023.09-red)](https://arxiv.org/abs/2309.07864) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/WooooDyy/LLM-Agent-Paper-List)
* **Ghost In The Minecraft: Generally Capable Agents For Open-World Environments Via Large Language Models With Text-Based Knowledge And Memory** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)](https://arxiv.org/abs/2305.17144) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OpenGVLab/GITM)
* **Mot: Memory-Of-Thought Enables Chatgpt To Self-Improve** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://aclanthology.org/2023.emnlp-main.392.pdf) 
* **Memgpt: Towards Llms As Operating Systems** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.08560) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://www.letta.com/)
* **Clin: A Continually Learning Language Agent For Rapid Task Adaptation And Generalization** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.10134) 
* **Open-Ended Instructable Embodied Agents With Memory-Augmented Large Language Models** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://arxiv.org/abs/2310.15127) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://helper-agent-llm.github.io/)
* **Think-In-Memory: Recalling And Post-Thinking Enable Llms With Long-Term Memory** [![arXiv](https://img.shields.io/badge/arXiv-2023.11-red)](https://arxiv.org/abs/2311.08719) 

### 1.2 Finetuning-based Methods
#### 1.2.1 Imitation Learning-based Methods 
* **Opencodereasoning: Advancing Data Distillation For Competitive Coding** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.01943) 
* **Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts In Multi-Objective Alignment** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.14354) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zyttt-coder/SIPO)
* **Enhancing Reasoning Capabilities Of Llms Via Principled Synthetic Logic Corpus** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2411.12498#:~:text=To%20address%20this%2C%20we%20propose%20Additional%20Logic%20Training,integrating%20symbolic%20logic%20theory%20and%20previous%20empirical%20insights.) 
* **Horizon-Length Prediction: Advancing Fill-In-The-Middle Capabilities For Code Generation With Lookahead Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.03103) 
* **Cooperative Strategic Planning Enhances Reasoning Capabilities In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.20007#:~:text=This%20paper%20proposes%20a%20novel%20cooperative%20multi-agent%20reasoning,agents%3A%20a%20planning%20agent%20and%20a%20reasoning%20agent.) 
* **Dualformer: Controllable Fast And Slow Thinking By Learning With Randomized Reasoning Traces** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.09918#:~:text=To%20address%20this%20challenge%2C%20we%20present%20Dualformer%2C%20a,parts%20of%20the%20traces%20are%20dropped%20during%20training.) 
* **Camphor: Collaborative Agents For Multi-Input Planning And High-Order Reasoning On Device** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.09407)
* **Selp: Generating Safe And Efficient Task Plans For Robot Agents With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.19471) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/lt-asset/selp)
* **Synatra: Turning Indirect Knowledge Into Direct Demonstrations For Digital Agents At Scale** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.15637)
* **Learning To Plan Long-Term For Language Modeling** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.00070)
* **Agentgen: Enhancing Planning Abilities For Large Language Model Based Agent Via Environment And Task Generation** [![arXiv](https://img.shields.io/badge/arXiv-2024.08-red)](https://arxiv.org/abs/2408.00764) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://agent-gen.github.io/)
* **Retrieve-Plan-Generation: An Iterative Planning And Answering Framework For Knowledge-Intensive LLM Generation** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.14979) 
* **Mixture-Of-Agents Enhances Large Language Model Capabilities** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.04692) 
* **Ask-Before-Plan: Proactive Language Agents For Real-World Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://aclanthology.org/2024.findings-emnlp.636.pdf)
* **Learning To Plan For Retrieval-Augmented Large Language Models From Knowledge Graphs** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.14282) 
* **Distilling Instruction-Following Abilities Of Large Language Models With Task-Aware Curriculum Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.13448)
* **Monte Carlo Tree Search Boosts Reasoning Via Iterative Preference Learning** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.00451) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/YuxiXie/MCTS-DPO)
* **Agent Planning With World Knowledge Model** [![arXiv](https://img.shields.io/badge/arXiv-2024-red)](https://arxiv.org/abs/2405.14205) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/WKM)
* **Stream Of Search (Sos): Learning To Search In Language** [![arXiv](https://img.shields.io/badge/arXiv-2024.04-red)](https://arxiv.org/abs/2404.03683) 
* **Agent-Flan: Designing Data And Methods Of Effective Agent Tuning For Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://arxiv.org/abs/2403.12881) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/InternLM/Agent-FLAN)
* **Knowagent: Knowledge-Augmented Planning For LLM-Based Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://arxiv.org/abs/2403.03101) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/KnowAgent)
* **Beyond A\*: Better Planning With Transformers Via Search Dynamics Bootstrapping** [![arXiv](https://img.shields.io/badge/arXiv-2024.02-red)](https://arxiv.org/abs/2402.14083) 
* **Autoact: Automatic Agent Learning From Scratch Via Self-Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.01-red)](https://arxiv.org/abs/2401.05268) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/AutoAct)
* **Adapting LLM Agents With Universal Feedback In Communication** [![ICMLW](https://img.shields.io/badge/ICMLW-2024-red)](https://arxiv.org/abs/2310.01444) 
* **Fireact: Toward Language Agent Fine-Tuning** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.05915) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://fireact-agent.github.io/)
* **You Only Look At Screens: Multimodal Chain-Of-Action Agents** [![arXiv](https://img.shields.io/badge/arXiv-2023.09-red)](https://arxiv.org/abs/2309.11436) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/cooelf/Auto-GUI)
* **Multimodal Web Navigation With Instruction-Finetuned Foundation Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)](https://arxiv.org/abs/2305.11854) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://sites.google.com/view/mm-webnav/)
* **Training Verifiers To Solve Math Word Problems** [![arXiv](https://img.shields.io/badge/arXiv-2021.10-red)](https://arxiv.org/abs/2110.14168)
#### 1.2.2 Feedback-based Methods
* **Gui-R1: A Generalist R1-Style Vision-Language Action Model For Gui Agents** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.10458) 
* **Infigui-R1: Advancing Multimodal Gui Agents From Reactive Actors To Deliberative Reasoners** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.14239) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Reallm-Labs/InfiGUI-R1)
* **Embodied-R: Collaborative Framework For Activating Embodied Spatial Reasoning In Foundation Models Via Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.12680) 
* **Retool: Reinforcement Learning For Strategic Tool Use In Llms** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.11536) 
* **Toolrl: Reward Is All Tool Learning Needs** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.13958) 
* **Otc: Optimal Tool Calls Via Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.14870)
* **Boost, Disentangle, And Customize: A Robust System2-To-System1 Pipeline For Code Generation** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.12492) 
* **Alphamaze: Enhancing Large Language Models' Spatial Intelligence Via Grpo** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.14669) 
* **Swe-Rl: Advancing LLM Reasoning Via Reinforcement Learning On Open Software Evolution** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.18449) 
* **Think Smarter Not Harder: Adaptive Reasoning With Inference Aware Optimization** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.17974) 
* **Error Classification Of Large Language Models On Math Word Problems: A Dynamically Adaptive Framework** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.15581) 
* **Closed-Loop Long-Horizon Robotic Planning Via Equilibrium Sequence Modeling** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.01440) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Singularity0104/equilibrium-planner)
* **Thinking Llms: General Instruction Following With Thought Generation** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.10630) 
* **Enhancing LLM Reasoning Via Critique Models With Test-Time And Training-Time Supervision** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)](https://arxiv.org/abs/2411.16579#:~:text=In%20this%20paper%2C%20we%20delve%20into%20a%20two-player,reasoning%20%28actor%29%20model%20during%20both%20test-time%20and%20train-time.) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://mathcritique.github.io/)
* **Webrl: Training LLM Web Agents Via Self-Evolving Online Curriculum Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Yu-Fangxu/FoR)
* **Can LLM Be A Good Path Planner Based On Prompt Engineering? Mitigating The Hallucination For Path Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.08-red)](https://arxiv.org/abs/2408.13184) 
* **Flow Of Reasoning: Training Llms For Divergent Problem Solving With Minimal Examples** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.05673) [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Rest-Mcts\*: LLM Self-Training Via Process Reward Guided Tree Search** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.03816) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/THUDM/ReST-MCTS)
* **Q\*: Improving Multi-Step Reasoning For Llms With Deliberative Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.14283) 
* **Learn Beyond The Answer: Training Language Models With Reflection For Mathematical Reasoning** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.12050) 
* **Chain Of Preference Optimization: Improving Chain-Of-Thought Reasoning In Llms** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.09136) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/sail-sg/CPO)
* **Self-Play Preference Optimization For Language Model Alignment** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.00675) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/uclaml/SPPO)
* **Training Language Models To Self-Correct Via Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.12917) 
* **Cpl: Critical Plan Step Learning Boosts LLM Generalization In Reasoning Tasks** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.08642)
* **Self-Playing Adversarial Language Game Enhances LLM Reasoning** [![arXiv](https://img.shields.io/badge/arXiv-2024.04-red)](https://arxiv.org/abs/2404.10642) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Linear95/SPAG)
* **Advancing LLM Reasoning Generalists With Preference Trees** [![arXiv](https://img.shields.io/badge/arXiv-2024.04-red)](https://arxiv.org/abs/2404.02078)
* **Autowebglm: A Large Language Model-Based Web Navigating Agent** [![SIGKDD](https://img.shields.io/badge/SIGKDD-2024-red)](https://arxiv.org/abs/2404.03648) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/THUDM/AutoWebGLM)
* **Llms In The Imaginarium: Tool Learning Through Simulated Trial And Error** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://arxiv.org/abs/2403.04746) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/simulated-trial-and-error)
* **Trial And Error: Exploration-Based Trajectory Optimization For LLM Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://arxiv.org/abs/2403.02502) 
* **React Meets Actre: When Language Agents Enjoy Training Data Autonomy** [![CoRR](https://img.shields.io/badge/CoRR-2024-red)](https://arxiv.org/abs/2403.14589#:~:text=In%20this%20work%2C%20we%20propose%20A%203%20T%2C,which%20explains%20the%20reason%20for%20an%20arbitrary%20action.) 
* **Learning Planning-Based Reasoning By Trajectories Collection And Process Reward Synthesizing** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2402.00658) 
* **Self-Play Fine-Tuning Converts Weak Language Models To Strong Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.01-red)](https://arxiv.org/abs/2401.01335) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/uclaml/SPIN)
* **Beyond Human Data: Scaling Self-Training For Problem-Solving With Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.12-red)](https://arxiv.org/abs/2312.06585) 
* **A Real-World Webagent With Planning, Long Context Understanding, And Program Synthesis** [![arXiv](https://img.shields.io/badge/arXiv-2023.07-red)](https://arxiv.org/abs/2307.12856) 
* **Language Models Can Teach Themselves To Program Better** [![arXiv](https://img.shields.io/badge/arXiv-2022.07-red)](https://arxiv.org/abs/2207.14502) 
* **Ui-R1: Enhancing Action Prediction Of Gui Agents By Reinforcement Learning** [![arXiv](https://img.shields.io/badge/arXiv-2025.03-red)](https://arxiv.org/abs/2503.21620v1)
### 1.3 Searching-based Methods
#### 1.3.1 Decomposition-based Methods
* **Verilogcoder: Autonomous Verilog Coding Agents With Graph-Based Planning And Abstract Syntax Tree (Ast)-Based Waveform Tracing Tool** [![AAAI](https://img.shields.io/badge/AAAI-2025-red)](https://arxiv.org/abs/2408.08927) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/NVlabs/VerilogCoder)
* **Agent-Oriented Planning In Multi-Agent Systems** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.02189) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/lalaliat/Agent-Oriented-Planning)
* **Optimizing Chain-Of-Thought Reasoning: Tackling Arranging Bottleneck Via Plan Augmentation** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.16812) 
* **Can We Further Elicit Reasoning In Llms? Critic-Guided Planning With Retrieval-Augmentation For Solving Challenging Tasks** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.01428#:~:text=To%20address%20this%2C%20we%20introduce%20Critic-guided%20planning%20with,a%20problem%20by%20iteratively%20selecting%20and%20executing%20sub-goals.) 
* **Oscar: Operating System Control Via State-Aware Reasoning And Re-Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.18963) 
* **Plan-Rag: Planning-Guided Retrieval Augmented Generation** [![arXiv](https://img.shields.io/badge/arXiv-2024-red)](https://arxiv.org/abs/2410.20753) 
* **Travelagent: An AI Assistant For Personalized Travel Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.08069) 
* **Meta-Task Planning For Language Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.16510v3) 
* **A Human-Like Reasoning Framework For Multi-Phases Planning Task With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.18208) 
* **Urbanllm: Autonomous Urban Activity Planning And Management With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.12360) 
* **Paradise: Evaluating Implicit Planning Skills Of Language Models With Procedural Warnings And Tips Dataset** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://aclanthology.org/2024.findings-acl.599/)
* **Rada: Retrieval-Augmented Web Agent Planning With Llms** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://aclanthology.org/2024.findings-acl.802/) 
* **Thoughts To Target: Enhance Planning For Target-Driven Conversation** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://aclanthology.org/2024.emnlp-main.1175/) 
* **Personal Large Language Model Agents: A Case Study On Tailored Travel Planning** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://aclanthology.org/2024.emnlp-industry.37/) 
* **Strength Lies In Differences! Improving Strategy Planning For Non-Collaborative Dialogues Via Diversified User Simulation** [![arXiv](https://img.shields.io/badge/arXiv-2024.03-red)](https://aclanthology.org/2024.emnlp-main.26/) 
* **Protrix: Building Models For Planning And Reasoning Over Tables With Sentence Context** [![arXiv](https://img.shields.io/badge/arXiv-2024-red)](https://arxiv.org/abs/2403.02177) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/WilliamZR/ProTrix)
* **Adapt: As-Needed Decomposition And Planning With Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.11-red)](https://arxiv.org/abs/2311.05772)
* **Hugginggpt: Solving AI Tasks With Chatgpt And Its Friends In Hugging Face** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2303.17580) 
* **Distilling Script Knowledge From Large Language Models For Constrained Language Planning** [![ACL](https://img.shields.io/badge/ACL-2023-red)](https://aclanthology.org/2023.acl-long.236/) 
* **Interpretable Math Word Problem Solution Generation Via Step-By-Step Planning** [![ACL](https://img.shields.io/badge/ACL-2023-red)](https://aclanthology.org/2023.acl-long.379/)
* **Deductive Additivity For Planning Of Natural Language Proofs** [![ACL](https://img.shields.io/badge/ACL-2023-red)](https://aclanthology.org/2023.nlrse-1.11/)
* **Least-To-Most Prompting Enables Complex Reasoning In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2022.05-red)](https://arxiv.org/abs/2205.10625) 
* **Msi-Agent: Incorporating Multi-Scale Insight Into Embodied Agents For Superior Planning And Decision-Making** [![ACL](https://img.shields.io/badge/ACK-2024-red)](https://aclanthology.org/2024.emnlp-main.38/)
#### 1.3.2 Exploration-based Methods
* **Worldcoder, A Model-Based LLM Agent: Building World Models By Writing Code And Interacting With The Environment** [![NIPS](https://img.shields.io/badge/NIPS-2025-red)](https://arxiv.org/abs/2402.12275) 
* **Scaling Autonomous Agents Via Automatic Reward Modeling And Planning** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2502.12130) [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Flow-Of-Options: Diversified And Improved LLM Reasoning By Thinking Through Options** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.12929) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/flagshippioneering/Flow-of-Options)
* **Adaptive Graph Of Thoughts: Test-Time Adaptive Reasoning Unifying Chain, Tree, And Graph Structures** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.05078) 
* **Step Back To Leap Forward: Self-Backtracking For Boosting Reasoning Of Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.04404)
* **Doubly Robust Monte Carlo Tree Search** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.01672) 
* **Coat: Chain-Of-Associated-Thoughts Framework For Enhancing Large Language Models Reasoning** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.02390)
* **Qlass: Boosting Language Agent Inference Via Q-Guided Stepwise Search** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.02584) 
* **Reflective Planning: Vision-Language Models For Multi-Stage Long-Horizon Robotic Manipulation** [![arXiv](https://img.shields.io/badge/arXiv-2025.02-red)](https://arxiv.org/abs/2502.16707) 
* **Codetree: Agent-Guided Tree Search For Code Generation With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)](https://arxiv.org/abs/2411.04329) 
* **Is Your LLM Secretly A World Model Of The Internet? Model-Based Planning For Web Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.11-red)](https://arxiv.org/abs/2411.06559) 
* **Reasonplanner: Enhancing Autonomous Planning In Dynamic Environments With Temporal Knowledge Graphs And Llms** [![arXiv](https://img.shields.io/badge/arXiv-2024.10-red)](https://arxiv.org/abs/2410.09252) 
* **System-1. X: Learning To Balance Fast And Slow Planning With Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.14414)
* **Tree Search For Language Model Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.01476) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://jykoh.com/search-agents)
* **LLM-A\*: Large Language Model Enhanced Incremental Heuristic Search On Path Planning** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2407.02511) 
* **Webpilot: A Versatile And Autonomous Multi-Agent System For Web Task Execution With Strategic Exploration** [![arXiv](https://img.shields.io/badge/arXiv-2024.08-red)](https://arxiv.org/abs/2408.15978) 
* **Graph Of Thoughts: Solving Elaborate Problems With Large Language Models** [![AAAI](https://img.shields.io/badge/AAAI-2024-red)]() [![Code](https://img.shields.io/badge/Code-GitHub-blue)]()
* **Tree-Planner: Efficient Close-Loop Task Planning With Large Language Models** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2310.08582)
* **Thoughtsculpt: Reasoning With Intermediate Revision And Search** [![arXiv](https://img.shields.io/badge/arXiv-2024.04-red)](https://aclanthology.org/2025.findings-naacl.428/)
* **Alphamath Almost Zero: Process Supervision Without Process** [![arXiv](https://img.shields.io/badge/arXiv-2024.05-red)](https://arxiv.org/abs/2405.03553) 
* **Search-In-The-Chain: Interactively Enhancing Large Language Models With Search For Knowledge-Intensive Tasks** [![WWW](https://img.shields.io/badge/WWW-2024-red)](https://arxiv.org/abs/2304.14732) 
* **Toolchain\*: Efficient Action Space Navigation In Large Language Models With A\* Search** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.13227) 
* **Language Agent Tree Search Unifies Reasoning Acting And Planning In Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.04406) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/lapisrocks/LanguageAgentTreeSearch)
* **Thought Propagation: An Analogical Approach To Complex Reasoning With Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2308.09687) 
* **Algorithm Of Thoughts: Enhancing Exploration Of Ideas In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.08-red)](https://arxiv.org/abs/2308.10379) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://algorithm-of-thoughts.github.io/)
* **Toolllm: Facilitating Large Language Models To Master 16000+ Real-World Apis** [![arXiv](https://img.shields.io/badge/arXiv-2023.07-red)](https://arxiv.org/abs/2307.16789) 
* **Tree Of Thoughts: Deliberate Problem Solving With Large Language Models** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2305.10601) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/princeton-nlp/tree-of-thought-llm)
* **Avis: Autonomous Visual Information Seeking With Large Language Model Agent** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2306.08129) 
* **Large Language Models As Commonsense Knowledge For Large-Scale Task Planning** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2305.14078) 
* **Reasoning With Language Model Is Planning With World Model** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)](https://aclanthology.org/2023.emnlp-main.507/) 
* **Let'S Verify Step By Step** [![ICLR](https://img.shields.io/badge/ICLR-2023-red)](https://arxiv.org/abs/2305.20050)
* **Solving Math Word Problems With Process-And Outcome-Based Feedback** [![arXiv](https://img.shields.io/badge/arXiv-2022.11-red)](https://arxiv.org/abs/2211.14275)
#### 1.3.3 Decoding-based Methods
* **Self-Evaluation Guided Beam Search For Reasoning** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2305.00633) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://guideddecoding.github.io/)
* **Chain-Of-Thought Reasoning Without Prompting** [![arXiv](https://img.shields.io/badge/arXiv-2024.02-red)](https://arxiv.org/abs/2402.10200) 
* **Flap: Flow-Adhering Planning With Constrained Decoding In Llms** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2403.05766) 
* **Contrastive Decoding Improves Reasoning In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2023.09-red)](https://arxiv.org/abs/2309.09117)
* **Don't Throw Away Your Value Model! Generating More Preferable Text With Value-Guided Monte-Carlo Tree Search Decoding** [![CoLM](https://img.shields.io/badge/CoLM-2024-red)](https://arxiv.org/abs/2309.15028) 
* **Grounded Decoding: Guiding Text Generation With Grounded Models For Embodied Agents** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2303.00855) 
* **Non-Myopic Generation Of Language Model For Reasoning And Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024-red)](https://arxiv.org/abs/2410.17195) 
* **Chain-Of-Thought Prompting Elicits Reasoning In Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2022-red)](https://arxiv.org/abs/2201.11903)
  
## 2 Planning Evaluation
### 2.1 Datasets
#### 2.1.1 Digital Scenarios
##### 2.1.1.1 Web Navigation
* **WebLINX: Real-World Website Navigation with Multi-Turn Dialogue** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2402.05930) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/McGill-NLP/weblinx)
* **WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2401.13919) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/MinorJerry/WebVoyager)
* **VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2401.13649) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/web-arena-x/visualwebarena)
* **WebArena: A Realistic Web Environment for Building Autonomous Agents** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2307.13854) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/web-arena-x/webarena)
* **Mind2Web: Towards a Generalist Agent for the Web** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2306.06070) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OSU-NLP-Group/Mind2Web)
* **WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents** [![NIPS](https://img.shields.io/badge/NIPS-2022-red)](https://arxiv.org/abs/2207.01206) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/princeton-nlp/WebShop)
* **Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration** [![ICLR](https://img.shields.io/badge/ICLR-2018-red)](https://arxiv.org/abs/1802.08802) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Farama-Foundation/miniwob-plusplus)

##### 2.1.1.2 Mobile Navigation
* **A3: Android Agent Arena for Mobile GUI Agents**  [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.01149) 
* **AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2405.14573) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-research/android_world)
* **On the Effects of Data Scale on UI Control Agents** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2406.03679) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-research/google-research/tree/master/android_control)
* **Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction** [![arXiv](https://img.shields.io/badge/arXiv-2023.05-red)](https://arxiv.org/abs/2305.08144) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/X-LANCE/Mobile-Env)
* **Android in the Wild: A Large-Scale Dataset for Android Device Control** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2307.10088) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-research/google-research/tree/master/android_in_the_wild)
* **META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI** [![EMNLP](https://img.shields.io/badge/EMNLP-2022-red)](https://arxiv.org/abs/2205.11029) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://x-lance.github.io/META-GUI-Leaderboard/)
* **Mapping Natural Language Instructions to Mobile UI Action Sequences** [![ACL](https://img.shields.io/badge/ACL-2020-red)](https://arxiv.org/abs/2005.03776) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-research/google-research/tree/master/seq2act)

##### 2.1.1.3 Desktop Navigation
* **AgentStudio: A Toolkit for Building General Virtual Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2403.17918) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/ltzheng/agent-studio)
* **OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2404.07972) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/xlang-ai/OSWorld)
* **TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.14161) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/TheAgentCompany/TheAgentCompany)
* **Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale** [![arXiv](https://img.shields.io/badge/arXiv-2024.09-red)](https://arxiv.org/abs/2409.08264) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/WindowsAgentArena)
* **WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2403.07718) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/ServiceNow/WorkArena)

#### 2.1.2 Embodied Scenarios
##### 2.1.2.1 Household Robot
* **PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2411.00081) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](http://github.com/facebookresearch/partnr-planner)
* **LangSuitE: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2406.16294) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/bigai-nlco/langsuite)
* **ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2410.03907) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/HKUST-KnowComp/ActPlan-1K)
* **Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2410.07166) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/embodied-agent-interface/embodied-agent-interface)
* **LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2402.08178) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/lbaa2022/LLMTaskPlanning)
* **GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation** [![CVPR](https://img.shields.io/badge/CVPR-2024-red)](https://arxiv.org/abs/2404.06609) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Ram81/goat-bench)
* **ScienceWorld: Is your Agent Smarter than a 5th Grader?** [![EMNLP](https://img.shields.io/badge/EMNLP-2022-red)](https://arxiv.org/abs/2203.07540) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/allenai/ScienceWorld)
* **BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation** [![CoRL](https://img.shields.io/badge/CoRL-2022-red)](https://arxiv.org/abs/2403.09227) [![Code](https://img.shields.io/badge/Code-Dataset-blue)](https://behavior.stanford.edu/behavior-1k)
* **ALFWorld: Aligning Text and Embodied Environments for Interactive Learning** [![ICLR](https://img.shields.io/badge/ICLR-2021-red)](https://arxiv.org/abs/2010.03768) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/alfworld/alfworld)
* **Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration** [![ICLR](https://img.shields.io/badge/ICLR-2021-red)](https://arxiv.org/abs/2010.09890) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/xavierpuigf/watch_and_help)
* **ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks** [![CVPR](https://img.shields.io/badge/CVPR-2020-red)](https://arxiv.org/abs/1912.01734) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/askforalfred/alfred)
* **VirtualHome: Simulating Household Activities via Programs** [![CVPR](https://img.shields.io/badge/CVPR-2018-red)](https://arxiv.org/abs/1806.07011) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/xavierpuigf/virtualhome)

##### 2.1.2.2 Manipulation Robot
* **EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents** [![ICML](https://img.shields.io/badge/ICML-2025-red)](https://arxiv.org/abs/2502.09560) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/EmbodiedBench/EmbodiedBench)
* **VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.18194) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OpenMOSS/VLABench)
* **RoCo: Dialectic Multi-Robot Collaboration with Large Language Models** [![ICRA](https://img.shields.io/badge/ICRA-2024-red)](https://arxiv.org/abs/2307.04738) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/MandiZhao/robot-collab)
* **VIMA: General Robot Manipulation with Multimodal Prompts** [![ICML](https://img.shields.io/badge/ICML-2023-red)](https://arxiv.org/abs/2210.03094) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/vimalabs/VIMA)
* **VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation** [![NIPS](https://img.shields.io/badge/NIPS-2022-red)](https://arxiv.org/abs/2206.08522) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/eric-ai-lab/vlmbench)

##### 2.1.2.3 Minecraft Robot
* **Plancraft: an evaluation dataset for planning with LLM agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.21033) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/gautierdag/plancraft)
* **TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.05255) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/teamcraft-bench/teamcraft)
* **Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2402.16801) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/MichaelTMatthews/Craftax)
* **MinePlanner: A Benchmark for Long-Horizon Planning in Large Minecraft Worlds** [![ICAPS](https://img.shields.io/badge/ICAPS-2024-red)](https://arxiv.org/abs/2312.12891) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/IretonLiu/mine-pddl/)
* **MindAgent: Emergent Gaming Interaction** [![NAACL](https://img.shields.io/badge/NAACL-2024-red)](https://arxiv.org/abs/2309.09971) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/mindagent/mindagent)
* **MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge** [![NIPS](https://img.shields.io/badge/NIPS-2022-red)](https://arxiv.org/abs/2206.08853) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/MineDojo/MineDojo)
* **On the Utility of Learning about Humans for Human-AI Coordination** [![NIPS](https://img.shields.io/badge/NIPS-2019-red)](https://arxiv.org/abs/1910.05789) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/HumanCompatibleAI/overcooked_ai)

##### 2.1.2.4 Autonomous Driving
* **AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via Reinforcement Learning and Reasoning** [![arXiv](https://img.shields.io/badge/arXiv-2025.03-red)](https://arxiv.org/abs/2503.07608) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/hustvl/AlphaDrive)
* **PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2402.15527) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/pkunlp-icler/PCA-EVAL)

#### 2.1.3 Everyday Scenarios
##### 2.1.3.1 Travel Planning
* **ChinaTravel: A Real-World Benchmark for Language Agents in Chinese Travel Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.12-red)](https://arxiv.org/abs/2412.13682) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/LAMDASZ-ML/ChinaTravel)
* **NATURAL PLAN: Benchmarking LLMs on Natural Language Planning** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.04520) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/google-deepmind/natural-plan)
* **TravelPlanner: A Benchmark for Real-World Planning with Language Agents** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2402.01622) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OSU-NLP-Group/TravelPlanner)

##### 2.1.3.2 Workflow
* **Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2411.17188) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Dongping-Chen/ISG)
* **Benchmarking Agentic Workflow Generation** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2410.07869) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/WorfBench)
* **FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2406.14884) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Justherozen/FlowBench)
* **Open Grounded Planning: Challenges and Benchmark Construction** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2406.02903) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Shiguang-Guo/Open-Grounded-Planning)
* **TaskBench: Benchmarking Large Language Models for Task Automation** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2311.18760) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/JARVIS/tree/main/taskbench)
* **TaskLAMA: Probing the Complex Task Understanding of Language Models** [![AAAI](https://img.shields.io/badge/AAAI-2024-red)](https://arxiv.org/abs/2308.15299) [![Code](https://img.shields.io/badge/Code-Dataset-blue)](https://storage.googleapis.com/gresearch/tasklama/tasklama.zip)
* **Multimodal Procedural Planning via Dual Text-Image Prompting** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2305.01795) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/YujieLu10/TIP)
* **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2303.17580) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/JARVIS/tree/main/hugginggpt)

##### 2.1.3.3 Tool Calling
* **ToolComp: A Multi-Tool Reasoning & Process Supervision Benchmark** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.01290)
* **ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities** [![NAACL](https://img.shields.io/badge/NAACL-2025-red)](https://arxiv.org/abs/2408.04682) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/apple/ToolSandbox)
* **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2307.16789) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OpenBMB/ToolBench)
* **AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2407.18901) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/stonybrooknlp/appworld)
* **ToolTalk: Evaluating Tool-Usage in a Conversational Setting** [![arXiv](https://img.shields.io/badge/arXiv-2023.11-red)](https://arxiv.org/abs/2311.10775) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/microsoft/ToolTalk)
* **API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://arxiv.org/abs/2304.08244) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank)

##### 2.1.3.4 Code Generation
* **Training Software Engineering Agents and Verifiers with SWE-Gym** [![ICML](https://img.shields.io/badge/ICML-2025-red)](https://arxiv.org/abs/2412.21139) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/SWE-Gym/SWE-Gym)
* **SWE-bench: Can Language Models Resolve Real-World GitHub Issues?** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2310.06770) [![Code](https://img.shields.io/badge/Code-Dataset-blue)](https://www.swebench.com/)
* **Evaluating Large Language Models Trained on Code** [![arXiv](https://img.shields.io/badge/arXiv-2021.07-red)](https://arxiv.org/abs/2107.03374) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/openai/human-eval)

##### 2.1.3.5 Game Playing
* **VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.01863) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning)
* **PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2206.10498) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/karthikv792/LLMs-Planning/tree/main/plan-bench)
* **BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning** [![ICLR](https://img.shields.io/badge/ICLR-2019-red)](https://arxiv.org/abs/1810.08272) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/mila-iqia/babyai/tree/iclr19)
* **TextWorld: A Learning Environment for Text-based Games** [![IJCAI](https://img.shields.io/badge/IJCAI-2018-red)](https://arxiv.org/abs/1806.11532) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/microsoft/TextWorld)

#### 2.1.4 Vertical Scenarios
##### 2.1.4.1 Machine Learning
* **MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2410.07095) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/openai/mle-bench/)

##### 2.1.4.2 AI Research
* **CycleResearcher: Improving Automated Research via Automated Review** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2411.00816) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://wengsyx.github.io/Researcher/)
* **ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.10291)

##### 2.1.4.3 Biological Research
* **BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology** [![EMNLP](https://img.shields.io/badge/EMNLP-2023-red)](https://arxiv.org/abs/2310.10632) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/bioplanner/bioplanner)

##### 2.1.4.4 Financial Simulation
* **Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena** [![arXiv](https://img.shields.io/badge/arXiv-2023.10-red)](https://arxiv.org/abs/2310.05746) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/jiangjiechen/auction-arena)

##### 2.1.4.5 Interior Design
* **DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.15723) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/plstory/DS2D)
* **Tell2Design: A Dataset for Language-Guided Floor Plan Generation** [![ACL](https://img.shields.io/badge/ACL-2023-red)](https://arxiv.org/abs/2311.15941) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/LengSicong/Tell2Design)

##### 2.1.4.6 Comprehensive
* **VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2408.06327) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/VisualAgentBench)
* **AgentBench: Evaluating LLMs as Agents** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2308.03688) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/AgentBench)

### 2.2 Evaluation Metrics

<p align="center">
  <img src="docs/sankeydiagram.png" width="95%" height="95%" />
</p>
The corresponding relationship between planning evaluation metrics and some typical planning datasets. The left three columns represent evaluation metrics of different granularities, while the rightmost column denotes the dataset.

### 2.3 Performance Comparisons
#### 2.3.1 Web Navigation Performance
* **TongUI: Building Generalized GUI Agents by Learning from Multimodal Web Tutorials** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.12679) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/TongUI-agent/TongUI-agent?tab=readme-ov-file)
* **SpiritSight Agent: Advanced GUI Agent with One Look** [![CVPR](https://img.shields.io/badge/CVPR-2025-red)](https://arxiv.org/abs/2503.03196) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://hzhiyuan.github.io/SpiritSight-Agent/)
* **UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis** [![arXiv](https://img.shields.io/badge/arXiv-2025.04-red)](https://arxiv.org/abs/2504.11257)
* **MP-GUI: Modality Perception with MLLMs for GUI Understanding** [![CVPR](https://img.shields.io/badge/CVPR-2025-red)](https://arxiv.org/abs/2503.14021) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/BigTaige/MP-GUI)
* **Magma: A Foundation Model for Multimodal AI Agents** [![CVPR](https://img.shields.io/badge/CVPR-2025-red)](https://arxiv.org/abs/2502.13130) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/microsoft/Magma)
* **Digi-Q: Learning Q-Value Functions for Training Device-Control Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2502.15760) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/DigiRL-agent/digiq)
* **MiniCPM-V: A GPT-4V Level MLLM on Your Phone** [![CVPR](https://img.shields.io/badge/CVPR-2025-red)](https://arxiv.org/abs/2408.01800) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/OpenBMB/MiniCPM-o)
* **Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2410.05243) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/OSU-NLP-Group/UGround)
* **UI-TARS: Pioneering Automated GUI Interaction with Native Agents** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.12326) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/bytedance/UI-TARS)
* **Tree Search for Language Model Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.07-red)](https://arxiv.org/abs/2407.01476) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/kohjingyu/search-agents)
* **GUICourse: From General Vision Language Models to Versatile GUI Agents** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.11317) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/RUCBM/GUICourse)
* **GPT-4V(ision) is a Generalist Web Agent, if Grounded** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2401.01614) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/OSU-NLP-Group/SeeAct)
* **SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2401.10935) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/njucckevin/SeeClick)
* **CogAgent: A Visual Language Model for GUI Agents** [![CVPR](https://img.shields.io/badge/CVPR-2024-red)](https://arxiv.org/abs/2312.08914) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/CogAgent)
* **Agent Lumos: Unified and Modular Training for Open-Source Language Agents** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2311.05657) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/allenai/lumos)
* **Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control** [![ICLR](https://img.shields.io/badge/ICLR-2024-red)](https://arxiv.org/abs/2306.07863) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/ltzheng/Synapse)
* **AgentTuning: Enabling Generalized Agent Abilities for LLMs** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2310.12823) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/AgentTuning)
* **Mind2Web: Towards a Generalist Agent for the Web** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2306.06070) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/OSU-NLP-Group/Mind2Web)


<p align="center">
  <img src="docs/WebNavigationPerformance.png" width="95%" height="95%" />
</p>
The performance comparison of different models and methods in web navigation.The value of Mind2Web is the average step success rate of the three subsets.The value of Webarena is task successrate.The value of AITW is step success rate of the subsets.The value of ScreenSpot is step success rate.

#### 2.3.2 Embodied Scenarios Performance
* **ATLaS: Agent Tuning via Learning Critical Steps** [![arXiv](https://img.shields.io/badge/arXiv-2025.03-red)](https://arxiv.org/abs/2503.02197)
* **DebFlow: Automating Agent Creation via Agent Debate** [![CoLM](https://img.shields.io/badge/CoLM-2025-red)](https://arxiv.org/abs/2503.23781)
* **AgentRefine: Enhancing Agent Generalization through Refinement Tuning** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2501.01702) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Fu-Dayuan/AgentRefine)
* **KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents** [![NAACL](https://img.shields.io/badge/NAACL-2025-red)](https://arxiv.org/abs/2403.03101) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/KnowAgent)
* **Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training** [![arXiv](https://img.shields.io/badge/arXiv-2025.01-red)](https://arxiv.org/abs/2501.11425) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/ByteDance-Seed/Agent-R)
* **AgentGym: Evolving Large Language Model-based Agents across Diverse Environments** [![arXiv](https://img.shields.io/badge/arXiv-2024.06-red)](https://arxiv.org/abs/2406.04151) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/WooooDyy/AgentGym)
* **Agent Planning with World Knowledge Model** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2405.14205) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/zjunlp/WKM)
* **Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2403.02502) [![Code](https://img.shields.io/badge/Code-GitHub-blue)](https://github.com/Yifan-Song793/ETO)
* **AgentTuning: Enabling Generalized Agent Abilities for LLMs** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2310.12823) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/THUDM/AgentTuning)

<p align="center">
  <img src="docs/EmbodiedScenariosPerformance.png" width="65%" height="65%" />
</p>
The performance comparison of different models and methods in embodied.This refers to the average of seen and unseen in the original paper, or the value reported in the original paper.

## 3 Analysis And Interpretation
### 3.1 External Interpretation
* **Revealing the Barriers of Language Agents in Planning** [![NAACL](https://img.shields.io/badge/NAACL-2025-red)](https://arxiv.org/abs/2410.12409) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/hsaest/Agent-Planning-Analysis)
* **To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning** [![ICLR](https://img.shields.io/badge/ICLR-2025-red)](https://arxiv.org/abs/2409.12183) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/Zayne-sprague/To-CoT-or-not-to-CoT)
* **The pitfalls of next-token prediction** [![ICML](https://img.shields.io/badge/ICML-2024-red)](https://arxiv.org/abs/2403.06963) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/gregorbachmann/Next-Token-Failures)
* **Chain of Thoughtlessness? An Analysis of CoT in Planning** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2405.04776) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/karthikv792/cot-planning)
* **Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models** [![arXiv](https://img.shields.io/badge/arXiv-2024.02-red)](https://arxiv.org/abs/2402.12563) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/MBZUAI-CLeaR/IoE-Prompting)
* **Small Language Models Need Strong Verifiers to Self-Correct Reasoning** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2404.17140) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/yunx-z/SCORE)
* **A Theoretical Understanding of Self-Correction through In-context Alignment** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2405.18634) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/yifeiwang77/Self-Correction)
* **Self-Refine: Iterative Refinement with Self-Feedback** [![NIPS](https://img.shields.io/badge/NIPS-2023-red)](https://arxiv.org/abs/2303.17651) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/madaan/self-refine)

### 3.2 Internal Interpretation
* **Do Large Language Models Latently Perform Multi-Hop Reasoning?** [![ACL](https://img.shields.io/badge/ACL-2024-red)](https://arxiv.org/abs/2402.16837) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/google-deepmind/latent-multi-hop-reasoning)
* **Do language models plan ahead for future tokens?** [![CoLM](https://img.shields.io/badge/CoLM-2024-red)](https://arxiv.org/abs/2404.00859) [![Code](https://img.shields.io/badge/Code-Github-blue)](https://github.com/wiwu2390/FutureGPT2-public)
* **Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models** [![EMNLP](https://img.shields.io/badge/EMNLP-2024-red)](https://arxiv.org/abs/2406.16033)
* **ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2405.09220)
* **Iteration Head: A Mechanistic Study of Chain-of-Thought** [![NIPS](https://img.shields.io/badge/NIPS-2024-red)](https://arxiv.org/abs/2406.02128)

<p align="center">
  <img src="docs/interpretation.png" width="50%" height="50%" />
</p>
The illustration of external interpretation and internal interpretation, respectively.
